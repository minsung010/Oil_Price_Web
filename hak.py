# train_flask_model.py
import os
import pandas as pd
import numpy as np
from glob import glob
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Concatenate, BatchNormalization, Flatten
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback
import tensorflow as tf
import matplotlib.pyplot as plt
import joblib
import time

# ----------------- GPU ì„¤ì • (GPU Configuration) -----------------
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥ (GPU available): {[gpu.name for gpu in gpus]}")
    except RuntimeError as e:
        print(e)
else:
    print("âš ï¸ GPU ë¯¸íƒì§€ (No GPU detected) â€” CPUë¡œ ì‹¤í–‰ ì¤‘ (Using CPU, slower)")

# ----------------- ê²½ë¡œ ì„¤ì • (Path Settings) -----------------
DATA_DIR = "monthly_csvs"
SEQ_LEN = 60
EPOCHS = 30
BATCH_SIZE = 128
MODEL_PATH = "models/global_lstm.keras"
SCALER_PATH = "scalers/global_scaler.save"
ENCODER_PATH = "scalers/uni_encoder.save"
PLOT_PATH = "static/plots/global_model_loss.png"
OUT_CSV = "prediction_summary.csv"

os.makedirs("models", exist_ok=True)
os.makedirs("scalers", exist_ok=True)
os.makedirs("static/plots", exist_ok=True)

# ----------------- CSV ë³‘í•© (Merge CSV files) -----------------
file_list = glob(os.path.join(DATA_DIR, "*.csv"))
all_data = []
for f in file_list:
    try:
        df = pd.read_csv(f, encoding='cp949')
        all_data.append(df)
    except Exception as e:
        print(f"[ê²½ê³  Warning] {f} ì½ê¸° ì‹¤íŒ¨ (Failed to read):", e)

data = pd.concat(all_data, ignore_index=True)
data = data.rename(columns={"ë²ˆí˜¸": "UNI_ID", "ê¸°ê°„": "DATE", "íœ˜ë°œìœ ": "B027", "ê²½ìœ ": "D047"})
data['DATE'] = pd.to_datetime(data['DATE'], format='%Y%m%d')
data = data.sort_values(['UNI_ID', 'DATE'])
data[['B027', 'D047']] = data.groupby('UNI_ID')[['B027', 'D047']].ffill()

print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ (Data loaded) â€” {len(data['UNI_ID'].unique())}ê°œ ì£¼ìœ ì†Œ (stations)")

# ----------------- ì¸ì½”ë”© & ìŠ¤ì¼€ì¼ë§ (Encoding & Scaling) -----------------
encoder = LabelEncoder()
data['UNI_ENC'] = encoder.fit_transform(data['UNI_ID'])
scaler = MinMaxScaler()
data['B027_scaled'] = scaler.fit_transform(data[['B027']])

joblib.dump(scaler, SCALER_PATH)
joblib.dump(encoder, ENCODER_PATH)

# ----------------- ì‹œí€€ìŠ¤ ìƒì„± (Create sequences for LSTM) -----------------
def create_global_sequences(df, seq_len=SEQ_LEN):
    X_seq, X_id, y = [], [], []
    for uid in df['UNI_ENC'].unique():
        sub = df[df['UNI_ENC'] == uid]
        prices = sub['B027_scaled'].values
        for i in range(len(prices) - seq_len):
            X_seq.append(prices[i:i + seq_len])
            X_id.append(uid)
            y.append(prices[i + seq_len])
    return np.array(X_seq), np.array(X_id), np.array(y)

X_seq, X_id, y = create_global_sequences(data)
print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ (Sequence generation done): {len(X_seq):,}ê°œ ìƒ˜í”Œ (samples)")

# ----------------- í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (Train/Validation split) -----------------
split_idx = int(len(X_seq) * 0.9)
X_seq_train, X_seq_val = X_seq[:split_idx], X_seq[split_idx:]
X_id_train, X_id_val = X_id[:split_idx], X_id[split_idx:]
y_train, y_val = y[:split_idx], y[split_idx:]

X_seq_train = X_seq_train.reshape((X_seq_train.shape[0], SEQ_LEN, 1))
X_seq_val = X_seq_val.reshape((X_seq_val.shape[0], SEQ_LEN, 1))

# ----------------- í•™ìŠµ ì§„í–‰ë¥  ì½œë°± (Progress + ETA) -----------------
class TimeHistory(Callback):
    def on_train_begin(self, logs=None):
        self.epoch_times = []
        self.start_time = time.time()

    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_start = time.time()

    def on_epoch_end(self, epoch, logs=None):
        elapsed = time.time() - self.epoch_start
        self.epoch_times.append(elapsed)
        avg_time = np.mean(self.epoch_times)
        remaining = avg_time * (self.params['epochs'] - (epoch + 1))
        print(f"â± Epoch {epoch+1}/{self.params['epochs']} ì™„ë£Œ â€” ê±¸ë¦° ì‹œê°„: {elapsed:.1f}s, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")

time_callback = TimeHistory()

# ----------------- ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë˜ëŠ” ìƒˆë¡œ ìƒì„± (Load or Build Model) -----------------
if os.path.exists(MODEL_PATH):
    print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (Resuming training from saved model)")
    model = tf.keras.models.load_model(MODEL_PATH)
else:
    print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Starting fresh training)")
    id_input = Input(shape=(1,), name="station_id")
    id_embed = Embedding(input_dim=len(encoder.classes_), output_dim=8)(id_input)
    id_flat = Flatten()(id_embed)
    id_dense = Dense(8, activation='relu')(id_flat)

    seq_input = Input(shape=(SEQ_LEN, 1), name="price_sequence")
    x = LSTM(128, return_sequences=True)(seq_input)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    x = LSTM(128)(x)
    x = Dropout(0.3)(x)

    merged = Concatenate()([x, id_dense])
    out = Dense(1)(merged)

    model = Model(inputs=[seq_input, id_input], outputs=out)
    model.compile(optimizer='adam', loss='mse')

# ----------------- ì½œë°± ì„¤ì • (Callbacks) -----------------
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True)

# ----------------- ëª¨ë¸ í•™ìŠµ (Train the Model) -----------------
history = model.fit(
    {"price_sequence": X_seq_train, "station_id": X_id_train},
    y_train,
    validation_data=({"price_sequence": X_seq_val, "station_id": X_id_val}, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[early_stop, checkpoint, time_callback],
    verbose=1
)

# ----------------- ì„±ëŠ¥ í‰ê°€ (Performance Evaluation) -----------------
y_pred = model.predict({"price_sequence": X_seq_val, "station_id": X_id_val}, verbose=0).flatten()
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
mae = mean_absolute_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print("\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Model Performance Evaluation)")
print(f"  - RMSE: {rmse:.6f}")
print(f"  - MAE : {mae:.6f}")
print(f"  - RÂ²  : {r2:.6f}")

# ----------------- í•œê¸€ í°íŠ¸ ì„¤ì • (Fix Korean Font Warning) -----------------
import matplotlib
matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # Windowsìš©
matplotlib.rcParams['axes.unicode_minus'] = False

# ----------------- ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥ (Save Loss Graph) -----------------
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Train Loss (í›ˆë ¨ ì†ì‹¤)')
plt.plot(history.history['val_loss'], label='Validation Loss (ê²€ì¦ ì†ì‹¤)')
plt.legend()
plt.title("Global LSTM Loss â€” ì†ì‹¤ ê·¸ë˜í”„")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.tight_layout()
plt.savefig(PLOT_PATH)
plt.close()

# ----------------- 7ì¼ ì˜ˆì¸¡ (7-day Forecast per Station) -----------------
results = []
for uni_id in data['UNI_ID'].unique():
    sub = data[data['UNI_ID'] == uni_id].sort_values('DATE')
    enc_id = encoder.transform([uni_id])[0]
    seq = sub['B027_scaled'].values[-SEQ_LEN:].reshape(1, SEQ_LEN, 1)
    preds_scaled = []
    for _ in range(7):
        next_val = model.predict({"price_sequence": seq, "station_id": np.array([[enc_id]])}, verbose=0)[0, 0]
        preds_scaled.append(next_val)
        seq = np.append(seq[:, 1:, :], [[[next_val]]], axis=1)
    preds = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()

    results.append({
        "UNI_ID": uni_id,
        "pred_day7_price": float(preds[-1]),
        "avg_predicted_price_7days": float(np.mean(preds))
    })

pd.DataFrame(results).to_csv(OUT_CSV, index=False, encoding='utf-8-sig')
print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(results)}ê°œ ì£¼ìœ ì†Œ")
print(f"âœ… ê²°ê³¼ CSV: {OUT_CSV}")
print(f"âœ… ì†ì‹¤ ê·¸ë˜í”„: {PLOT_PATH}")
print(f"âœ… ëª¨ë¸ ì €ì¥ë¨: {MODEL_PATH}")
